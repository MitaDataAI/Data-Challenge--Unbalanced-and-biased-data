{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe98ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "038b22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "674eace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('df_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19565ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:, :768]\n",
    "Y_train = df_train.iloc[:, 768]\n",
    "gender_train = df_train.iloc[:, 769]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43cdd7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = pd.read_csv('df_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92c3191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = df_validation.iloc[:, :768]\n",
    "Y_validation = df_validation.iloc[:, 768]\n",
    "gender_validation = df_validation.iloc[:, 769]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf7241",
   "metadata": {},
   "source": [
    "# Combinaison de X_train et gender_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51cc3f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dafe325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, Y_train, X_validation, Y_validation, gender_validation):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle et évalue les performances sur l'ensemble de validation,\n",
    "    y compris l'évaluation de la justice à l'aide des fonctions fournies dans evaluator.py.\n",
    "    \"\"\"\n",
    "    # Entraîner le modèle\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de validation\n",
    "    predictions = model.predict(X_validation)\n",
    "\n",
    "    # Évaluation des métriques de performance et de justice en utilisant evaluator.py\n",
    "    eval_scores, confusion_matrices = evaluator.gap_eval_scores(\n",
    "        predictions, Y_validation, gender_validation, metrics=['TPR', 'FPR', 'PPR']\n",
    "    )\n",
    "\n",
    "    # Utilisation directe de 'accuracy' tel que retourné par gap_eval_scores\n",
    "    accuracy = eval_scores['accuracy']\n",
    "\n",
    "    # Les métriques de performance et de justice sont extraites directement des résultats d'évaluation\n",
    "    performance_metrics = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Macro F1 Score': eval_scores['macro_fscore'],\n",
    "        'Micro F1 Score': eval_scores['micro_fscore'],\n",
    "    }\n",
    "\n",
    "    fairness_metrics = {\n",
    "        'TPR_GAP': eval_scores.get('TPR_GAP', 0),\n",
    "        'FPR_GAP': eval_scores.get('FPR_GAP', 0),\n",
    "        'PPR_GAP': eval_scores.get('PPR_GAP', 0),\n",
    "    }\n",
    "\n",
    "    final_score = (performance_metrics['Macro F1 Score'] + (1 - fairness_metrics['TPR_GAP'])) / 2\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"Performance Metrics:\")\n",
    "    for key, value in performance_metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(\"\\nFairness Metrics:\")\n",
    "    for key, value in fairness_metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    print(f\"\\nFinal Score: {final_score}\")\n",
    "    print(f\"Number of Estimators: {len(model.estimators_)}\")\n",
    "\n",
    "    return {\n",
    "        'performance_metrics': performance_metrics,\n",
    "        'fairness_metrics': fairness_metrics,\n",
    "        'number_of_estimators': len(model.estimators_),\n",
    "        'final_score': final_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bd9e53",
   "metadata": {},
   "source": [
    "Cette fonction a été mis sous format .py avec l'ajout de la validation croisée pour entrainer le modèle en fonction de notre critère de précision et d'équité. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acf29776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation avec OneVsRestClassifier et SGDClassifier\n",
    "ovr_clf = OneVsRestClassifier(SGDClassifier(random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b4ac990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics:\n",
      "Accuracy: 0.7387387387387387\n",
      "Macro F1 Score: 0.6257162124859735\n",
      "Micro F1 Score: 0.7387387387387387\n",
      "\n",
      "Fairness Metrics:\n",
      "TPR_GAP: 0.18877768365076086\n",
      "FPR_GAP: 0.008478595565504461\n",
      "PPR_GAP: 0.025465046424129084\n",
      "\n",
      "Final Score: 0.7184692644176063\n",
      "Number of Estimators: 28\n"
     ]
    }
   ],
   "source": [
    "results = train_and_evaluate(ovr_clf, X_train, Y_train, X_validation, Y_validation, gender_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ced7c9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'performance_metrics': {'Accuracy': 0.7387387387387387,\n",
       "  'Macro F1 Score': 0.6257162124859735,\n",
       "  'Micro F1 Score': 0.7387387387387387},\n",
       " 'fairness_metrics': {'TPR_GAP': 0.18877768365076086,\n",
       "  'FPR_GAP': 0.008478595565504461,\n",
       "  'PPR_GAP': 0.025465046424129084},\n",
       " 'final_score': 0.7184692644176063,\n",
       " 'number_of_estimators': 28}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19773edf",
   "metadata": {},
   "source": [
    "Pour gérer à la fois le déséquilibre de classe et l'iniquité tout en maintenant une bonne précision, On derva prioriser le Macro F1 Score pour le déséquilibre de classe, les métriques TPR_GAP, FPR_GAP, et PPR_GAP pour l'évaluation de l'iniquité, et utiliser l'Accuracy avec prudence, en complément d'autres métriques plus informatives dans ces contextes spécifiques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
